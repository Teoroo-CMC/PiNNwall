{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinn.io import sparse_batch\n",
    "from glob import glob\n",
    "import os, re, warnings\n",
    "from ase.data import atomic_numbers\n",
    "from pinn.io.base import list_loader\n",
    "from scipy.io import FortranFile\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from pol_models_ewald import *\n",
    "from pol_utils_ewald import *\n",
    "import csv\n",
    "import numpy\n",
    "import time\n",
    "import math\n",
    "# Check that all the imports are necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to write the path where the input files should be found\n",
    "path_to_files = '/home/thomas/MW_pinet_interface/check_existence'\n",
    "\n",
    "data_file = path_to_files + '/data.inpt'\n",
    "runtime_file = path_to_files + '/runtime.inpt'\n",
    "filelist = glob(data_file)\n",
    "\n",
    "# Path to the location of the models\n",
    "path_to_models = '/home/thomas/ML_model/final_models'\n",
    "\n",
    "# List all the models used in an array of strings\n",
    "model_list = ['eem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_elec_check=10\n",
    "# Future improvement: use the Python Metalwalls interface to get it automatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existence(path_to_files,data_file,runtime_file,model_list,path_to_models):\n",
    "\n",
    "    \"\"\" Check the existence of the different inputs: data.inpt file, models used\n",
    "\n",
    "    Args:\n",
    "       path_to_files: path to input (data.inpt and runtime.inpt) and output (pinnwall.out and hessian_matrix.inpt)\n",
    "       data_file: data.inpt file\n",
    "       runtime_file: runtime.inpt file\n",
    "       model_list: list of model used to compute the CRK\n",
    "       path_to_models: path to the directory where the ML models are stored\n",
    "       \n",
    "    Returns:\n",
    "        If a required file does not exist, an error message is written in pinnwall.out and the execution stops\n",
    "        If a model in model_list is not supported by PiNN, an error message is written in pinnwall.out and the execution stops\n",
    "    \"\"\"\n",
    "    \n",
    "    default_list = ['acks2','eem','etainv','local']\n",
    "    \n",
    "    if not (os.path.isfile(data_file)):\n",
    "        fname = path_to_files + '/pinnwall.out'\n",
    "        output = open(fname, 'w')\n",
    "        output.write(\"PiNNWALL started\\n\\n\")\n",
    "        output.write(\"Working directory :\\n\")\n",
    "        output.write(\"{0:50s}\\n\".format(path_to_files))\n",
    "        output.write(\"ERROR : data.inpt file does not exist in working directory\\n\")\n",
    "        output.write(\"exit\\n\")\n",
    "        raise SystemExit(\"Execution ended with error\")\n",
    "        \n",
    "    if not (os.path.isfile(runtime_file)):\n",
    "        fname = path_to_files + '/pinnwall.out'\n",
    "        output = open(fname, 'w')\n",
    "        output.write(\"PiNNWALL started\\n\\n\")\n",
    "        output.write(\"Working directory :\\n\")\n",
    "        output.write(\"{0:50s}\\n\".format(path_to_files))\n",
    "        output.write(\"ERROR : runtime.inpt file does not exist in working directory\\n\")\n",
    "        output.write(\"exit\\n\")\n",
    "        raise SystemExit(\"Execution ended with error\")\n",
    "    \n",
    "    for model in model_list:\n",
    "        if not (model in default_list):\n",
    "            fname = path_to_files + '/pinnwall.out'\n",
    "            output = open(fname, 'w')\n",
    "            output.write(\"PiNNWALL started\\n\\n\")\n",
    "            output.write(\"Working directory :\\n\")\n",
    "            output.write(\"{0:50s}\\n\".format(path_to_files))\n",
    "            output.write(\"ERROR : the following model is not supported by PiNN\\n\")\n",
    "            output.write(\"{0:8s} \".format(model))\n",
    "            output.write(\"List of models supported by PiNN:\\n\")\n",
    "            for CDFT_method in model_list:\n",
    "                output.write(\"{0:8s} \".format(CDFT_method))\n",
    "                output.write(\"\\n\")\n",
    "            output.write(\"exit\\n\")\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_spec = {\n",
    "    'elems': {'dtype':  tf.int32,   'shape': [None]},\n",
    "    'coord': {'dtype':  tf.float32, 'shape': [None, 3]},\n",
    "    'ptensor': {'dtype': tf.float32, 'shape': [3, 3]},\n",
    "    'cell':   {'dtype': tf.float32, 'shape': [3, 3]},\n",
    "    'coord_check': {'dtype':  tf.float64, 'shape': [None, 3]}}\n",
    "\n",
    "@list_loader(ds_spec=ds_spec, pbc=True)\n",
    "def load_data_inpt(fname):\n",
    "\n",
    "    \"\"\"Load the data from the Metalwalls configuration file data.inpt\n",
    "\n",
    "    Args:\n",
    "       fname: data.inpt file\n",
    "       \n",
    "    Returns:\n",
    "        coord: the coordinate of the electrode atoms\n",
    "        ptensor:\n",
    "        cell: cell parameters, for an orthorombic box\n",
    "        coord_check: an array containing the position of the first ten electrode atoms\n",
    "        \n",
    "    Issues:\n",
    "        For now, the runtime.inpt fiel is not read. It should be added in the future to get the tolerance and cutoff to\n",
    "        compute the Ewald parameters, check the finite field type as well as the Gaussian width parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(fname) as run:\n",
    "        for (linenum, line) in enumerate(run):\n",
    "            if (line.lstrip()).startswith(\"num_electrode_atoms\"):\n",
    "                nelec = int(line.split()[1])\n",
    "            if (line.lstrip()).startswith(\"num_atoms\"):\n",
    "                nat = int(line.split()[1])\n",
    "            if (line.lstrip()).startswith(\"# box\"):\n",
    "                line2 = run.readline()\n",
    "                cellx = float(line2.split()[0])\n",
    "                celly = float(line2.split()[1])\n",
    "                cellz = float(line2.split()[2])\n",
    "            if (line.lstrip()).startswith(\"# coordinates\"):\n",
    "                nheader = linenum + 2\n",
    "\n",
    "    nions=nat-nelec\n",
    "\n",
    "    atname,x,y,z = np.loadtxt(fname, skiprows=nions+nheader, max_rows=nelec, unpack=True, dtype='U')\n",
    "\n",
    "    x = np.asfarray(x) * 0.52917721092\n",
    "    y = np.asfarray(y) * 0.52917721092\n",
    "    z = np.asfarray(z) * 0.52917721092\n",
    "    \n",
    "    elems = [atomic_number(a) for a in atname]\n",
    "\n",
    "    \n",
    "    coord = np.column_stack((x,y,z))\n",
    "    \n",
    "    coord_check = coord [:n_elec_check,:] / 0.52917721092\n",
    "\n",
    "    pol = np.zeros((3,3))\n",
    "\n",
    "    \n",
    "    cellx = cellx * 0.52917721092\n",
    "    celly = celly * 0.52917721092\n",
    "    cellz = cellz * 0.52917721092\n",
    "    cell = [[cellx,0,0],[0,celly,0],[0,0,cellz]]\n",
    "\n",
    "    applied_D = [False,False,False]\n",
    "    \n",
    "    return {'coord': coord, 'elems':elems, 'ptensor': pol, 'cell': cell, 'coord_check': coord_check}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_number(atomic_name):\n",
    "    \"\"\"Returns the atomic number list associated with the electrode atoms\n",
    "\n",
    "    Args:\n",
    "       atomic_name: list of strings corresponding to the atom names in the data.inpt file.\n",
    "       \n",
    "    Returns:\n",
    "        atomic_number: a list of atomic numbers corresponding to the electrode atoms\n",
    "       \n",
    "    Issue:\n",
    "        At the moment, I use the first string of the atomic name to get the atomic element because 1 electrode = 1 species and 1 species = 1 name, but I don't find that satisfying\n",
    "    \"\"\"\n",
    "    \n",
    "    if atomic_name.startswith(\"C\"):\n",
    "        atnumber = 6\n",
    "    if atomic_name.startswith(\"N\"):\n",
    "        atnumber = 7\n",
    "    if atomic_name.startswith(\"O\"):\n",
    "        atnumber = 8\n",
    "    if atomic_name.startswith(\"H\"):\n",
    "        atnumber = 1\n",
    "    if atomic_name.startswith(\"S\"):\n",
    "        atnumber = 16\n",
    "\n",
    "    return atnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_runtime_inpt(fname):\n",
    "    \"\"\"Reads in the runtime file if the electric displacement is applied and in which direction\n",
    "\n",
    "    Args:\n",
    "       fname: path and name of the runtime.inpt file\n",
    "       \n",
    "    Returns:\n",
    "        applied_D: a list of logical variable saying if the electric displacement is applied in the x, y and z directions respectively\n",
    "       \n",
    "    Issue:\n",
    "        Under construction, will go back to it after the main issues are fixed. Potential improvement: use the python interface of Metalwalls?\n",
    "    \"\"\"\n",
    "    field_param = ''\n",
    "    with open(fname) as run:\n",
    "        for (linenum, line) in enumerate(run):\n",
    "            if (line.lstrip()).startswith(\"external_field\"):\n",
    "                field_param = np.loadtxt(fname, skiprows=linenum+1, unpack=True, dtype='U')\n",
    "            if (line.lstrip()).startswith(\"num_pbc\"):\n",
    "                pbc = int(line.split()[1])\n",
    "            if (line.lstrip()).startswith(\"coulomb_rtol\"):\n",
    "                rtol = float(line.split()[1])\n",
    "            if (line.lstrip()).startswith(\"coulomb_rcut\"):\n",
    "                rcut = float(line.split()[1])\n",
    "            if (line.lstrip()).startswith(\"coulomb_ktol\"):\n",
    "                ktol = float(line.split()[1])\n",
    "#    print(field_param)\n",
    "#    field_type = 'E'\n",
    "#    field_direction = [0,0,0]\n",
    "#    for i in range(len(field_param[:,0])):\n",
    "#        if field_param[i,0] == 'type':\n",
    "#            field_type=field_param[i,1]\n",
    "#        if field_param[i,0] == 'direction':\n",
    "#            field_direction = field_param[i,1:3]\n",
    "    \n",
    "#    applied_D = [False,False,False]\n",
    "#    for i in range(3):\n",
    "#        if field_type=='D' and not field_direction[i]==0:\n",
    "#            applied_D[i]=True\n",
    "        \n",
    "    return pbc,rcut,rtol,ktol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Ewald_parameters(box,rcut,rtol,ktol):\n",
    "    \n",
    "    \"\"\"Computes the Ewald summation parameters from the simulation cell parameters\n",
    "\n",
    "    Args:\n",
    "       box: list of strings corresponding to the atom names in the data.inpt file.\n",
    "       \n",
    "    Returns:\n",
    "        eta: Gaussian width used for the Ewald summation\n",
    "        rcut: cutoff for electrostatic interactions\n",
    "        kmax: maximum number of k vectors\n",
    "       \n",
    "    Issue:\n",
    "        At the moment, the cutoff is chosen as half of the smallest box dimension, and not taken from the Metalwalls input.\n",
    "        The maximum number of k vectors is the same in all the directions, even though the box is not cubic.\n",
    "        The Gaussian width is derived from a given tolerance, this should be read from the Metalwalls input\n",
    "    \"\"\"\n",
    "    \n",
    "    L = box        # box dimensions\n",
    "    \n",
    "    V = L[0,0]*L[1,1]*L[2,2]\n",
    "    acc = math.sqrt(-math.log(rtol)) # Desired accuracy\n",
    "    c = np.cbrt(1/(V))\n",
    "    eta = (1/(math.sqrt(2*math.pi)*c))/10.\n",
    "    rcut = acc*math.sqrt(2)*eta\n",
    "    kcut = math.sqrt(2)*acc/eta\n",
    "    kmax = np.math.ceil(kcut*np.amax(L)/(2*math.pi))\n",
    "\n",
    "    rcut =np.amin([L[0,0],L[1,1],L[2,2]])/2.\n",
    "    eta = rcut / math.sqrt(-math.log(rcut*ktol)*2)\n",
    "    kmax = np.math.ceil(math.sqrt(-math.log(ktol)*2)*rcut/(eta*math.pi))\n",
    "    \n",
    "    return eta, kmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-3/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-4/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-5/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-8/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-2/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-0/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-7/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-6/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-9', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-9', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-9/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/thomas/ML_model/final_models/qm7b-nofilter-eem-5E5-1/model.ckpt-500000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "check_existence(path_to_files,data_file,runtime_file,model_list,path_to_models)\n",
    "dataset = lambda: load_data_inpt(filelist)\n",
    "box = np.float64(next(dataset().as_numpy_iterator())['cell']) * 1.88973\n",
    "pbc,rcut,rtol,ktol = load_runtime_inpt(runtime_file)\n",
    "eta, kmax = get_Ewald_parameters(box,rcut,rtol,ktol)\n",
    "fname = path_to_files + '/pinnwall.out'\n",
    "output = open(fname, 'w')\n",
    "output.write(\"PiNNWALL started\\n\\n\")\n",
    "output.write(\"Working directory :\\n\")\n",
    "output.write(\"{0:50s}\\n\".format(path_to_files))\n",
    "output.write(\"Models used : \")\n",
    "for CDFT_method in model_list:\n",
    "    output.write(\"{0:8s} \".format(CDFT_method))\n",
    "output.write(\"\\n\")\n",
    "output.write(\"Ewald cutoff {0:8.3f}\\n\".format(rcut * 0.52917721092))\n",
    "output.write(\"Eta {0:8.3f}\\n\".format(eta))\n",
    "output.write(\"Maximum number of k points {0:d}\\n\\n\".format(kmax))\n",
    "for CDFT_method in model_list:\n",
    "    tmodel0 = time.time()\n",
    "    output.write(\"Start model {0:8s}\\n\".format(CDFT_method))\n",
    "    model_choice = path_to_models + '/*' + CDFT_method + '*'\n",
    "    #\n",
    "    avg_chi = []\n",
    "    for m in glob(model_choice):\n",
    "        model = get_model(m)\n",
    "        params = model.params.copy()\n",
    "        params['model']['params'].update(ewald_rc=rcut, ewald_kmax=kmax, ewald_eta=eta)\n",
    "        model = get_model(params)\n",
    "        pred = [out for out in \n",
    "                model.predict(lambda: dataset().apply(sparse_batch(1)))]\n",
    "\n",
    "        for c, prediction in enumerate(pred):\n",
    "            mat_chi = prediction['chi']\n",
    "            avg_chi.append(mat_chi)\n",
    "\n",
    "    average_chi = np.float64(np.average(avg_chi, axis=0))\n",
    "    \n",
    "    # Future improvement: Move the writing of the file in its own function?\n",
    "    coord_check = np.float64(next(dataset().as_numpy_iterator())['coord_check'])\n",
    "    \n",
    "    fname = path_to_files + '/hessian_matrix_' + CDFT_method + '.inpt'\n",
    "    # If the model list contains only one element, the matrix file is named data.inpt, otherwhise the model is specified in the file name\n",
    "    if len(model_list) == 1:\n",
    "        fname = path_to_files + '/hessian_matrix.inpt'\n",
    "    # The matrix file read by Metalwalls is a binary file\n",
    "    output.write(\"Generate CRK file :\\n\")\n",
    "    output.write(\"{0:50s}\\n\".format(fname))\n",
    "    f = FortranFile(fname, 'w')\n",
    "    f.write_record(n_elec_check)\n",
    "    f.write_record(coord_check.T)\n",
    "    f.write_record(-average_chi.T)\n",
    "    f.close()\n",
    "    tmodel1 = time.time()\n",
    "    output.write(\"End model {0:8s}\\n\\n\".format(CDFT_method))\n",
    "\n",
    "output.write(\"\\n End of PiNNWALL\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
